# build a 8-bit quantizer‘
我们这个自定义的量化器的执行过程为：
1、加载原模型
2、将原模型中的 torch.nn.Linear 替换为我们的自定义 Linear
3、量化
4、forward

为简化，本文基于对称量化实现量化器。我们将构建量化器分为以下几个步骤:
1、创建 W8A16LinearLayer 类用于存储 scale 和 8-bit 的权重
2、用 W8A16LinearLayer 替换 torch.nn.Linear
3、构建量化器，并端到端量化一个模型
4、在多场景下测试我们的量化器，并研究8-bit量化对不同模型的影响
## 一、W8A16LinearLayer
1、实现 w8_a16_forward 函数
由前文所述，使用量化后的数据进行推理，我们先要对量化后的权重反量化，然后进行计算。
代入Linear 也就是
```python
def w8_a16_forward(weight, input, scales, bias=None):
    dequantized_weight = weight.to(input.dtype) * scales
    output = torch.nn.functional.linear(input, dequantized_weight)

    if bias is not None:
        output = output + bias
    return output
```
2、实现 W8A16LinearLayer 的 init
我们需要存储weights、scales 和 bias
```python
def __init__(self, in_features, out_features, 
                bias=True, dtype=torch.float32):
    super().__init__()

    self.register_buffer("int8_weights",
            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8)
    )
    
    self.register_buffer("scales", 
                            torch.randn((out_features), dtype=dtype))
    
    if bias:
        self.register_buffer("bias", 
                                torch.randn((1, out_features), dtype=dtype))
    else:
        self.bias = None
```
3、实现 W8A16LinearLayer 的 forward
```python
def forward(self, input):
    return w8_a16_forward(self.int8_weights, input, self.scales, self.bias)
```
4、实现 W8A16LinearLayer 的 quantize
```python
def quantize(self, weights):
    w_fp32 = weights.clone().to(torch.float32)

    scales = w_fp32.abs().max(dim=-1).values / 127  # 按列取max 计算scale，所以是per channel量化
    scales = scales.to(weights.dtype)

    int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)

    self.int8_weights = int8_weights
    self.scales = scales
```
## 二、替换 torch.nn.Linear
```python
def replace_linear_with_target_and_quantize(module, target_class, module_name_to_exclude):
    for name, child in module.named_children():
        # 如果是 nn.Linear 并且没有在 exclude module里，则将 nn.Linear 替换成 target_class，也就是 W8A16LinearLayer
        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):
            old_bias = child.bias
            old_weight = child.weight

            new_module = target_class(child.in_features, 
                                      child.out_features, 
                                      old_bias is not None, 
                                      child.weight.dtype)
            setattr(module, name, new_module)

            getattr(module, name).quantize(old_weight)

            if old_bias is not None:
              getattr(module, name).bias = old_bias
        else:
            # Recursively call the function for nested modules
            replace_linear_with_target_and_quantize(child, 
                     target_class, module_name_to_exclude)
```

## 三、使用我们的量化器
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
model_id = "./models/Salesforce/codegen-350M-mono"

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)
tokenizer = AutoTokenizer.from_pretrained(model_id)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

print("************************* Model before ******************** \n", model)
replace_linear_with_target_and_quantize(model, W8A16LinearLayer, ["lm_head"])
print("************************* Model after ******************** \n", model)
```
运行输出为：
```shell
************************* Model before ********************
 CodeGenForCausalLM(
  (transformer): CodeGenModel(
    (wte): Embedding(51200, 1024)
    (drop): Dropout(p=0.0, inplace=False)
    (h): ModuleList(
      (0-19): 20 x CodeGenBlock(
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): CodeGenAttention(
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (mlp): CodeGenMLP(
          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)
          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)
)
************************* Model after ********************
 CodeGenForCausalLM(
  (transformer): CodeGenModel(
    (wte): Embedding(51200, 1024)
    (drop): Dropout(p=0.0, inplace=False)
    (h): ModuleList(
      (0-19): 20 x CodeGenBlock(
        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): CodeGenAttention(
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
          (qkv_proj): W8A16LinearLayer()
          (out_proj): W8A16LinearLayer()
        )
        (mlp): CodeGenMLP(
          (fc_in): W8A16LinearLayer()
          (fc_out): W8A16LinearLayer()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)
)
save memory:251.2896 MB
```
